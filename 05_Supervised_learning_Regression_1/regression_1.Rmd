---
title: "Supervised learning: Regression 1"
params:
  answers: true
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    pandoc_args: --output=regression_1_answers.html
  pdf_document:
    toc: true
    toc_depth: 1
    latex_engine: xelatex
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

# Introduction

In this practical, you will learn how to perform regression analysis, how to plot with confidence and prediction intervals, how to calculate MSE, perform train-test splits, and write a function for cross validation.

Just like in the practical at the end of chapter 3 of the ISLR book, we will use the `Boston` dataset, which is in the `MASS` package that comes with `R`.

```{r packages, warning = FALSE, message = FALSE}
library(ISLR)
library(MASS)
library(tidyverse)
```

# Regression in `R`

Regression is performed through the `lm()` function. It requires two arguments: a `formula` and `data`. A `formula` is a specific type of object that can be constructed like so:

```{r formula}
some_formula <- outcome ~ predictor_1 + predictor_2 
```

You can read it as "the outcome variable is a function of predictors 1 and 2". As with other objects, you can check its class and even convert it to other classes, such as a character vector:

```{r class}
class(some_formula)
as.character(some_formula)
```

You can estimate a linear model using `lm()` by specifying the outcome variable and the predictors in a formula and by inputting the dataset these variables should be taken from.

---

1. __Create a linear model object called `lm_ses` using the formula `medv ~ lstat` and the `Boston` dataset.__

---


```{r lm, include = params$answers}
lm_ses <- lm(formula = medv ~ lstat, data = Boston)
```

You have now trained a regression model with `medv` (housing value) as the outcome/dependent variable and `lstat` (socio-economic status) as the predictor / independent variable.

Remember that a regression estimates $\beta_0$ (the intercept) and $\beta_1$ (the slope) in the following equation:

$$\boldsymbol{y} = \beta_0 + \beta_1\cdot \boldsymbol{x}_1 + \boldsymbol{\epsilon}$$

---

2. __Use the function `coef()` to extract the intercept and slope from the `lm_ses` object. Interpret the slope coefficient.__

---

```{r coef, include = params$answers}
coef(lm_ses)

# for each point increase in lstat, the median housing value drops by 0.95
```


---

3. __Use `summary()` to get a summary of the `lm_ses` object. What do you see? You can use the help file `?summary.lm`.__

---


```{r sum, include = params$answers}
summary(lm_ses)
```


We now have a model object `lm_ses` that represents the formula

$$\text{medv}_i = 34.55 - 0.95 * \text{lstat}_i + \epsilon_i$$

With this object, we can predict a new `medv` value by inputting its `lstat` value. The `predict()` method enables us to do this for the `lstat` values in the original dataset.

---

4. __Save the predicted y values to a variable called `y_pred`__

---

```{r pred, include = params$answers}
y_pred <- predict(lm_ses)
```


---

5. __Create a scatter plot with `y_pred` mapped to the x position and the true y value (`Boston$medv`) mapped to the y value. What do you see? What would this plot look like if the fit were perfect?__

---

```{r predobs, include = params$answers}
tibble(pred = y_pred, 
       obs  = Boston$medv) %>% 
  ggplot(aes(x = pred, y = obs)) +
  geom_point() +
  theme_minimal() +
  geom_abline(slope = 1)

# I've added an ideal line where all the points would lie on if the 
# fit were perfect.
```

We can also generate predictions from new data using the `newdat` argument in the `predict()` method. For that, we need to prepare a data frame with new values for the original predictors. 

---

6. __Use the `seq()` function to generate a sequence of 1000 equally spaced values from 0 to 40. Store this vector in a data frame with (`data.frame()` or `tibble()`) as its column name `lstat`. Name the data frame `pred_dat`.__

---

```{r pred_dat, include = params$answers}
pred_dat <- tibble(lstat = seq(0, 40, length.out = 1000))
```

---

7. __Use the newly created data frame as the `newdata` argument to a `predict()` call for `lm_ses`. Store it in a variable named `y_pred_new`.__

---

```{r pred_ses, include = params$answers}
y_pred_new <- predict(lm_ses, newdata = pred_dat)
```

# Plotting lm() in `ggplot`

A good way of understanding your model is by visualising it. We are going to walk through the construction of a plot with a fit line and prediction / confidence intervals from an `lm` object.


---

8. __Create a scatter plot from the `Boston` dataset with `lstat` mapped to the x position and `medv` mapped to the y position. Store the plot in an object called `p_scatter`.__

---

```{r sctr, include = params$answers}
p_scatter <- 
  Boston %>% 
  ggplot(aes(x = lstat, y = medv)) +
  geom_point() +
  theme_minimal()

p_scatter
```

Now we're going to add a prediction line to this plot.

---

9. __Add the vector `y_pred_new` to the `pred_dat` data frame with the name `medv`.__

---

```{r col, include = params$answers}
# this can be done in several ways. Here are two possibilities:
# pred_dat$medv <- y_pred_new
pred_dat <- pred_dat %>% mutate(medv = y_pred_new)
```

---

10. __Add a geom_line() to `p_scatter`, with `pred_dat` as the `data` argument. What does this line represent?__

---

```{r line, include = params$answers}
p_scatter + geom_line(data = pred_dat)

# This line represents predicted values of medv for the values of lstat 
```

---

11. __The `interval` argument can be used to generate confidence or prediction intervals. Create a new object called `y_pred_95` using `predict()` (again with the `pred_dat` data) with the `interval` argument set to "confidence". What is in this object?__

---

```{r confint, include = params$answers}
y_pred_95 <- predict(lm_ses, newdata = pred_dat, interval = "confidence")

head(y_pred_95)

# it's a matrix with an estimate and a lower and an upper confidence interval.
```

---

12. __Create a data frame with 4 columns: `medv`, `lstat`, `lower`, and `upper`.__

---
```{r predframe, include = params$answers}
gg_pred <- tibble(
  lstat = pred_dat$lstat,
  medv  = y_pred_95[, 1],
  lower = y_pred_95[, 2],
  upper = y_pred_95[, 3]
)

gg_pred
```


---

13. __Add a `geom_ribbon()` to the plot with the data frame you just made. The ribbon geom requires three aesthetics: `x` (`lstat`, already mapped), `ymin` (`lower`), and `ymax` (`upper`). Add the ribbon below the `geom_line()` and the `geom_points()` of before to make sure those remain visible. Give it a nice colour and clean up the plot, too!__ 

```{r plot, include = params$answers}
# Create the plot
Boston %>% 
  ggplot(aes(x = lstat, y = medv)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), data = gg_pred, fill = "#00008b44") +
  geom_point(colour = "#883321") + 
  geom_line(data = pred_dat, colour = "#00008b", size = 1) +
  theme_minimal() + 
  labs(x    = "Proportion of low SES households",
       y    = "Median house value",
       title = "Boston house prices")
```

---

14. __Explain in your own words what the ribbon represents.__

---

```{r ribans, include = params$answers}
# The ribbon represents the 95% confidence interval of the fit line.
# The uncertainty in the estimates of the coefficients are taken into
# account with this ribbon. 

# You can think of it as:
# upon repeated sampling of data from the same population, at least 95% of
# the ribbons will contain the true fit line.
```


---

15. __Do the same thing, but now with the prediction interval instead of the confidence interval.__

---


```{r predint, include = params$answers}
# pred with pred interval
y_pred_95 <- predict(lm_ses, newdata = pred_dat, interval = "prediction")


# create the df
gg_pred <- tibble(
  lstat = pred_dat$lstat,
  medv  = y_pred_95[, 1],
  l95   = y_pred_95[, 2],
  u95   = y_pred_95[, 3]
)

# Create the plot
Boston %>% 
  ggplot(aes(x = lstat, y = medv)) + 
  geom_ribbon(aes(ymin = l95, ymax = u95), data = gg_pred, fill = "#00008b44") +
  geom_point(colour = "#883321") + 
  geom_line(data = pred_dat, colour = "#00008b", size = 1) +
  theme_minimal() + 
  labs(x     = "Proportion of low SES households",
       y     = "Median house value",
       title = "Boston house prices")


# You can look at ISLR p.81-82 for a discussion of prediction intervals
```

# Mean square error

---

16. __Write a function called `mse()` that takes in two vectors: true y values and predicted y values, and which outputs the mean square error.__ 

---

Start like so:

```{r mse, eval = FALSE}
mse <- function(y_true, y_pred) {
  # your function here
}
```

[Wikipedia](https://en.wikipedia.org/w/index.php?title=Mean_squared_error&oldid=857685443) may help for the formula.

```{r mse2, include = params$answers}
# there are many ways of doing this.
mse <- function(y_true, y_pred) {
  mean((y_true - y_pred)^2)
}
```

---

17. __Make sure your `mse()` function works correctly by running the following code.__

---

```{r msetest}
mse(1:10, 10:1)
```

You have now calculated the mean squared length of the dashed lines below.

```{r mseplot, echo = FALSE}
ggplot(data.frame(a = 1:10, b = 10:1, c = as.factor(letters[1:10])),
       aes(y = c)) +
  geom_segment(aes(x = a, xend = b, y = c, yend = c), lty = 2) +
  geom_point(aes(x = a), colour = "blue") +
  geom_point(aes(x = b), colour = "orange") + 
  coord_flip() +
  theme_minimal() +
  labs(x = "", y = "") +
  scale_x_continuous(breaks = 1:10) +
  theme(axis.text.x = element_blank())
```

---

18. __Calculate the mean square error of the `lm_ses` model. Use the `medv` column as `y_true` and use the `predict()` method to generate `y_pred`.__

---

```{r mse_ses, include = params$answers}
mse(Boston$medv, predict(lm_ses))
```

You have calculated the mean squared length of the dashed lines in the plot below.

```{r mseplot2, echo = FALSE}
Boston %>% 
  ggplot(aes(x = lstat, y = medv)) + 
  geom_segment(aes(xend = lstat, yend = predict(lm_ses)), lty = 2) +
  geom_point(colour = "#883321") + 
  geom_line(data = pred_dat, colour = "#00008b", size = 1) +
  theme_minimal() + 
  theme(legend.position = "bottom") +
  labs(x    = "Proportion of low SES households",
       y    = "Median house value",
       size = "Crime rate",
       title = "Boston house prices: errors")
```

# Train-validation-test split

Now we will use the `sample()` function to randomly select observations from the `Boston` dataset to go into a training, test, and validation set. The training set will be used to fit our model, the validation set will be used to calculate the out-of sample prediction error during model building, and the test set will be used to estimate the true out-of-sample MSE.

---

19. __The `Boston` dataset has `r nrow(Boston)` observations. Use `c()` and `rep()` to create a vector with 253 times the word "train", 152 times the word "validation", and 101 times the word "test". Call this vector `splits`.__

---


```{r set_splitting, include = params$answers}
splits <- c(rep("train", 253), rep("validation", 152), rep("test", 101))
```


---

20. __Use the function `sample()` to randomly order this vector and add it to the `Boston` dataset using `mutate()`. Assign the newly created dataset to a variable called `boston_master`.__

---

```{r set_splitting_2, include = params$answers}
boston_master <- Boston %>% mutate(splits = sample(splits))
```

---

21. __Now use `filter()` to create a training, validation, and test set from the `boston_master` data. Call these datasets `boston_train`, `boston_valid`, and `boston_test`.__

---

```{r set_splitting_3, include = params$answers}
boston_train <- boston_master %>% filter(splits == "train")
boston_valid <- boston_master %>% filter(splits == "validation")
boston_test  <- boston_master %>% filter(splits == "test")
```

We will set aside the `boston_test` dataset for now.

---

22. __Train a linear regression model called `model_1` using the training dataset. Use the formula `medv ~ lstat` like in the first `lm()` exercise. Use `summary()` to check that this object is as you expect.__

---

```{r lm1, include = params$answers}
model_1 <- lm(medv ~ lstat, data = boston_train)
summary(model_1)
```

---

23. __Calculate the MSE with this object. Save this value as `model_1_mse_train`.__

---


```{r lm1_mse_train, include = params$answers}
model_1_mse_train <- mse(y_true = boston_train$medv, y_pred = predict(model_1))
```


---

24. __Now calculate the MSE on the validation set and assign it to variable `model_1_mse_valid`. Hint: use the `newdata` argument in `predict()`.__

---

```{r lm1_mse_valid, include = params$answers}
model_1_mse_valid <- mse(y_true = boston_valid$medv, 
                         y_pred = predict(model_1, newdata = boston_valid))
```


This is the estimated out-of-sample mean squared error.

---

25. __Create a second model `model_2` for the train data which includes `age` and `tax` as predictors. Calculate the train and validation MSE.__

---

```{r lm2, include = params$answers}
model_2 <- lm(medv ~ lstat + age + tax, data = boston_train)
model_2_mse_train <- mse(y_true = boston_train$medv, y_pred = predict(model_2))
model_2_mse_valid <- mse(y_true = boston_valid$medv, 
                         y_pred = predict(model_2, newdata = boston_valid))
```

---

26. __Compare model 1 and model 2 in terms of their training and validation MSE. Which would you choose and why?__

---

```{r ans, include = params$answers}
# If you are interested in out-of-sample prediction, the 
# answer may depend on the random sampling of the rows in the
# dataset splitting: everyond has a different split. However, it
# is likely that model_2 has both lower training and validation MSE.
``` 

---

27. __Calculate the test MSE for the model of your choice in the previous question. What does this number tell you?__

---

```{r testmse, include = params$answers}
model_2_mse_test <- mse(y_true = boston_test$medv, 
                        y_pred = predict(model_2, newdata = boston_test))

# The estimate for the expected amount of error when predicting 
# the median value of a not previously seen town in Boston when 
# using this model is:

sqrt(model_2_mse_test)
```


# Programming exercise: cross-validation

This is an advanced exercise. Some components we have seen before in this and previous practicals, but some things will be completely new. Try to complete it by yourself, but don't worry if you get stuck. If you don't know about `for loops` in `R`, read up on those before you start the exercise. 


Use help in this order:

- R help files
- Internet search & stack exchange
- Your peers
- The answer, which shows one solution

You may also just read the answer and try to understand what happens in each step.

---

28. __Create a function that performs k-fold cross-validation for linear models.__

---

Inputs: 

- `formula`: a formula just as in the `lm()` function
- `dataset`: a data frame
- `k`: the number of folds for cross validation
- any other arguments you need necessary

Outputs:

- Mean square error averaged over folds


```{r crossval, include = params$answers}
# Just for reference, here is the mse() function once more
mse <- function(y_true, y_pred) mean((y_true - y_pred)^2)

cv_lm <- function(formula, dataset, k) {
  # We can do some error checking before starting the function
  stopifnot(is_formula(formula))       # formula must be a formula
  stopifnot(is.data.frame(dataset))    # dataset must be data frame
  stopifnot(is.integer(as.integer(k))) # k must be convertible to int
  
  # first, add a selection column to the dataset as before
  n_samples  <- nrow(dataset)
  select_vec <- rep(1:k, length.out = n_samples)
  data_split <- dataset %>% mutate(folds = sample(select_vec))
  
  # initialise an output vector of k mse values, which we 
  # will fill by using a _for loop_ going over each fold
  mses <- rep(0, k)
  
  # start the for loop
  for (i in 1:k) {
    # split the data in train and validation set
    data_train <- data_split %>% filter(folds != i)
    data_valid <- data_split %>% filter(folds == i)
    
    # calculate the model on this data
    model_i <- lm(formula = formula, data = data_train)
    
    # Extract the y column name from the formula
    y_column_name <- as.character(formula)[2]
    
    # calculate the mean square error and assign it to mses
    mses[i] <- mse(y_true = data_valid[[y_column_name]],
                   y_pred = predict(model_i, newdata = data_valid))
  }
  
  # now we have a vector of k mse values. All we need is to
  # return the mean mse!
  mean(mses)
}
```


---

29. __Use your function to perform 9-fold cross validation with a linear model with as its formula `medv ~ lstat + age + tax`. Compare it to a model with as formulat `medv ~ lstat + I(lstat^2) + age + tax`.__

---

```{r mse_cross, result = "hold", include = params$answers}
cv_lm(formula = medv ~ lstat + age + tax, dataset = Boston, k = 9)
cv_lm(formula = medv ~ lstat + I(lstat^2) + age + tax, dataset = Boston, k = 9)
```
